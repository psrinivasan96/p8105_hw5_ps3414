---
title: "Homework 5"
author: "Pavithra Srinivasan"
date: November 16, 2023
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(haven)
library(readxl)
library(dplyr)
library(tidyr)
library(broom)
library(rvest)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides.


## Problem 2

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time: 

Iterate file names and read in data for each subject and clean it:
```{r}

file_names = list.files(path = "./data1", full.names = TRUE)

#Import and combining files

tidy_df = file_names |> 
  map_dfr(read_csv, .id = "participant_id") |> 
  mutate(study_arm = print(dir(path = "./data1", include.dirs = FALSE))) |> 
  mutate(study_arm = str_remove(study_arm, ".csv")) |> 
  separate(study_arm, into = c("study_arm", "subject_id")) |> 
  select(participant_id, study_arm, subject_id, everything())

tidy_df |> 
  knitr::kable(digits = 2)

#Cleaning

tidy_df = tidy_df |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observations",
    names_prefix = "week_"
  ) |> 
  mutate(week = as.numeric(week))

tidy_df |> 
  head()

```

### Lets Visualize by creating a spaghetti plot

```{r}

spaghetti_plot = tidy_df |>
  group_by(subject_id) |> 
  ggplot(aes(x = week, y = observations)) + 
           geom_point(alpha = 0.7, aes(color = subject_id)) +  geom_line(alpha = 0.7, aes(color = subject_id)) +
          geom_smooth() +
  labs(
    title = "Observations of study participants over 8 weeks",
    x = "Week",
    y = "Observations"
  ) +
  facet_grid(. ~ study_arm)

spaghetti_plot
```

Interpretation: The Control arm is more constant for all the participants over the course of the 8 weeks. Whereas, there is an increase in the number of observations in the Experimental arm especially from week 2. Participant 4 had the most fluctuations in both the control and experimental groups, and participant 10 had the most erratic fluctuations in the experiment group. 

## Problem 3
n = 30
sigma = 5
alpha = 0.05

```{r}
set.seed(123)
```

Generate 5000 datasets from the model:

```{r}
sim_function = function(mu) {
  sim_data = tibble(rnorm(n = 30, mean = mu, sd = 5))
  
  sim_data |> 
    t.test() |> 
    broom::tidy() |> 
    select(estimate, p.value)
}

results = 
  expand_grid(
    mu = 0,
    iter = 1:5000) |> 
  mutate(estimate_df = map(mu, sim_function)) |> 
  unnest(estimate_df)
```

Repeat the above for Î¼={1,2,3,4,5,6}:






